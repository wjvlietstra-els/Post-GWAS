{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the features for the network statistics for leave-SNP-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/vlietstraw/git/Post-GWAS/ENSEMBL_mappings.json\", \"r\") as fp:\n",
    "    ensembl_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph data\n",
    "with open(\"/Users/vlietstraw/git/Post-GWAS/unfiltered_protein_protein_interactions.csv\", 'rb') as input_file:\n",
    "    #next(input_file, '')   # skip a line\n",
    "    G = nx.read_edgelist(input_file, delimiter=',', nodetype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degrees of the nodes (first metric)\n",
    "degrees = dict(G.degree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of connections between nodes and disease genes (second metrics)\n",
    "def calculate1N(nodeID, diseaseProteins, graph):\n",
    "    if nodeID in diseaseProteins:\n",
    "        diseaseProteins.remove(nodeID)\n",
    "    neighbours = set(dict(graph[nodeID]).keys())\n",
    "    dp_neighbours = neighbours.intersection(diseaseProteins)\n",
    "    return len(dp_neighbours)/len(neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of indirect connectinos between nodes and disease genes (third metric)\n",
    "def calculate2N(nodeID, diseaseProteins, graph):\n",
    "    if nodeID in diseaseProteins:\n",
    "        diseaseProteins.remove(nodeID)\n",
    "    indirect_neighbours_dict = dict(nx.single_source_shortest_path_length(G, source = nodeID, cutoff = 2))\n",
    "    indirect_neighbours = pd.DataFrame({\"nodeID\" : indirect_neighbours_dict.keys(), \"pathLength\" : indirect_neighbours_dict.values()})\n",
    "    indirect_neighbours = indirect_neighbours[indirect_neighbours[\"pathLength\"] == 2]\n",
    "    dp_indirect_neighbours = set(indirect_neighbours[\"nodeID\"]).intersection(diseaseProteins)\n",
    "    if len(indirect_neighbours) > 0:\n",
    "        return len(dp_indirect_neighbours)/len(indirect_neighbours)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average distance to disease genes (fourth metric)\n",
    "def getAverageDPDistance(nodeID, diseaseProteins, graph):\n",
    "    if nodeID in diseaseProteins:\n",
    "        diseaseProteins.remove(nodeID)\n",
    "    shortestPaths = dict(nx.single_source_shortest_path_length(graph, source = nodeID))\n",
    "    dp_shortestPaths = [shortestPaths[x] if x in shortestPaths.keys() else float('inf') for x in diseaseProteins]\n",
    "    output = sum(dp_shortestPaths)/len(dp_shortestPaths)\n",
    "    if output != float('inf'):\n",
    "        return output\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive topology coefficient (fifth metric)\n",
    "def calculateTopologyCoeff(nodeID, diseaseProteins, graph):\n",
    "    coeffs = []\n",
    "    \n",
    "    candidate = set(dict(graph[nodeID]).keys())\n",
    "    for dis in diseaseProteins:\n",
    "        dp = set(dict(graph[dis]).keys())\n",
    "\n",
    "        overlap = dp.intersection(candidate)\n",
    "        if len(overlap) > 0:\n",
    "            coeffs.append(len(overlap) / min(len(dp), len(candidate)))    \n",
    "    if len(coeffs) > 0:\n",
    "        return sum(coeffs) / len(coeffs)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_algorithms = [\"LR\", \"SVM\", \"DT\", \"KNN1\", \"KNN3\", \"KNN5\", \"KNN7\", \"KNN9\", \"RF\"]\n",
    "all_bp_distances = [100, 500, 1000, 2000]\n",
    "refsets = [\"Teslovich\", \"DeRycke\", \"farashi\", \"farashi p-value cutoff\"]\n",
    "\n",
    "all_metrics = pd.DataFrame(list(product(refsets, all_bp_distances, ML_algorithms)), columns = [\"refset\", \"bp distance\", \"algorithm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting row 1 of 144\n",
      "Predicting row 2 of 144\n",
      "Predicting row 3 of 144\n",
      "Predicting row 4 of 144\n",
      "Predicting row 5 of 144\n",
      "Predicting row 6 of 144\n",
      "Predicting row 7 of 144\n",
      "Predicting row 8 of 144\n",
      "Predicting row 9 of 144\n",
      "Predicting row 10 of 144\n",
      "Predicting row 11 of 144\n",
      "Predicting row 12 of 144\n",
      "Predicting row 13 of 144\n",
      "Predicting row 14 of 144\n",
      "Predicting row 15 of 144\n",
      "Predicting row 16 of 144\n",
      "Predicting row 17 of 144\n",
      "Predicting row 18 of 144\n",
      "Predicting row 19 of 144\n",
      "Predicting row 20 of 144\n",
      "Predicting row 21 of 144\n",
      "Predicting row 22 of 144\n",
      "Predicting row 23 of 144\n",
      "Predicting row 24 of 144\n",
      "Predicting row 25 of 144\n",
      "Predicting row 26 of 144\n",
      "Predicting row 27 of 144\n",
      "Predicting row 28 of 144\n",
      "Predicting row 29 of 144\n",
      "Predicting row 30 of 144\n",
      "Predicting row 31 of 144\n",
      "Predicting row 32 of 144\n",
      "Predicting row 33 of 144\n",
      "Predicting row 34 of 144\n",
      "Predicting row 35 of 144\n",
      "Predicting row 36 of 144\n",
      "Predicting row 37 of 144\n",
      "Predicting row 38 of 144\n",
      "Predicting row 39 of 144\n",
      "Predicting row 40 of 144\n",
      "Predicting row 41 of 144\n",
      "Predicting row 42 of 144\n",
      "Predicting row 43 of 144\n",
      "Predicting row 44 of 144\n",
      "Predicting row 45 of 144\n",
      "Predicting row 46 of 144\n",
      "Predicting row 47 of 144\n",
      "Predicting row 48 of 144\n",
      "Predicting row 49 of 144\n",
      "Predicting row 50 of 144\n",
      "Predicting row 51 of 144\n",
      "Predicting row 52 of 144\n",
      "Predicting row 53 of 144\n",
      "Predicting row 54 of 144\n",
      "Predicting row 55 of 144\n",
      "Predicting row 56 of 144\n",
      "Predicting row 57 of 144\n",
      "Predicting row 58 of 144\n",
      "Predicting row 59 of 144\n",
      "Predicting row 60 of 144\n",
      "Predicting row 61 of 144\n",
      "Predicting row 62 of 144\n",
      "Predicting row 63 of 144\n",
      "Predicting row 64 of 144\n",
      "Predicting row 65 of 144\n",
      "Predicting row 66 of 144\n",
      "Predicting row 67 of 144\n",
      "Predicting row 68 of 144\n",
      "Predicting row 69 of 144\n",
      "Predicting row 70 of 144\n",
      "Predicting row 71 of 144\n",
      "Predicting row 72 of 144\n",
      "Predicting row 73 of 144\n",
      "Predicting row 74 of 144\n",
      "Predicting row 75 of 144\n",
      "Predicting row 76 of 144\n",
      "Predicting row 77 of 144\n",
      "Predicting row 78 of 144\n",
      "Predicting row 79 of 144\n",
      "Predicting row 80 of 144\n",
      "Predicting row 81 of 144\n",
      "Predicting row 82 of 144\n",
      "Predicting row 83 of 144\n",
      "Predicting row 84 of 144\n",
      "Predicting row 85 of 144\n",
      "Predicting row 86 of 144\n",
      "Predicting row 87 of 144\n",
      "Predicting row 88 of 144\n",
      "Predicting row 89 of 144\n",
      "Predicting row 90 of 144\n",
      "Predicting row 91 of 144\n",
      "Predicting row 92 of 144\n",
      "Predicting row 93 of 144\n",
      "Predicting row 94 of 144\n",
      "Predicting row 95 of 144\n",
      "Predicting row 96 of 144\n",
      "Predicting row 97 of 144\n",
      "Predicting row 98 of 144\n",
      "Predicting row 99 of 144\n",
      "Predicting row 100 of 144\n",
      "Predicting row 101 of 144\n",
      "Predicting row 102 of 144\n",
      "Predicting row 103 of 144\n",
      "Predicting row 104 of 144\n",
      "Predicting row 105 of 144\n",
      "Predicting row 106 of 144\n",
      "Predicting row 107 of 144\n",
      "Predicting row 108 of 144\n",
      "Predicting row 109 of 144\n",
      "Predicting row 110 of 144\n",
      "Predicting row 111 of 144\n",
      "Predicting row 112 of 144\n",
      "Predicting row 113 of 144\n",
      "Predicting row 114 of 144\n",
      "Predicting row 115 of 144\n",
      "Predicting row 116 of 144\n",
      "Predicting row 117 of 144\n",
      "Predicting row 118 of 144\n",
      "Predicting row 119 of 144\n",
      "Predicting row 120 of 144\n",
      "Predicting row 121 of 144\n",
      "Predicting row 122 of 144\n",
      "Predicting row 123 of 144\n",
      "Predicting row 124 of 144\n",
      "Predicting row 125 of 144\n",
      "Predicting row 126 of 144\n",
      "Predicting row 127 of 144\n",
      "Predicting row 128 of 144\n",
      "Predicting row 129 of 144\n",
      "Predicting row 130 of 144\n",
      "Predicting row 131 of 144\n",
      "Predicting row 132 of 144\n",
      "Predicting row 133 of 144\n",
      "Predicting row 134 of 144\n",
      "Predicting row 135 of 144\n",
      "Predicting row 136 of 144\n",
      "Predicting row 137 of 144\n",
      "Predicting row 138 of 144\n",
      "Predicting row 139 of 144\n",
      "Predicting row 140 of 144\n",
      "Predicting row 141 of 144\n",
      "Predicting row 142 of 144\n",
      "Predicting row 143 of 144\n",
      "Predicting row 144 of 144\n"
     ]
    }
   ],
   "source": [
    "#Initialize emtpy variables\n",
    "distance_history = 0\n",
    "\n",
    "for am_index, am_values in all_metrics.iterrows():\n",
    "    print(\"Predicting row \" + str(am_index + 1) + \" of \" + str(len(all_metrics)))\n",
    "    \n",
    "    if distance_history != am_values[\"bp distance\"]:\n",
    "        if am_values[\"refset\"] == \"farashi\":\n",
    "            ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Farashi/Farashi full 2000000 bp distance no pvalue filtering.csv\")\n",
    "\n",
    "        if am_values[\"refset\"] == \"farashi p-value cutoff\":\n",
    "            ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Farashi/Farashi full 2000000 bp distance no pvalue filtering.csv\")\n",
    "            ref = ref[ref[\"GWAS/eQTL p-value¥\"] <= float(\"5e-8\")]\n",
    "\n",
    "        if am_values[\"refset\"] == \"DeRycke\":\n",
    "            ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/DeRycke/DeRycke reference set.csv\")\n",
    "            ref.columns = [\"SNP ID\", \"chromosome\", \"location\", \"gene_ids\", \"gene name\", \"gene start\", \"gene stop\", \"Diff expression\", \"Class\", \"bp distance absolute\", \"bp distance\", \"Gene rank\"]\n",
    "\n",
    "        if am_values[\"refset\"] == \"Teslovich\":\n",
    "            ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Teslovich/Teslovich reference set.csv\")\n",
    "            ref.columns = [\"SNP ID\", \"chromosome\", \"location\", \"P\", \"gene_ids\", \"gene name\", \"gene start\", \"gene stop\", \"Class\", \"bp distance absolute\", \"bp distance\", \"Gene rank\"]\n",
    "\n",
    "        ref[\"nodeID\"] = [ensembl_dict[x] if x in ensembl_dict.keys() else None for x in ref[\"gene_ids\"]]\n",
    "\n",
    "        # Set bp distance cutoff\n",
    "        max_bp_distance = am_values[\"bp distance\"]\n",
    "        max_bp_distance = max_bp_distance * 1000\n",
    "        ref = ref[ref[\"bp distance absolute\"] <= max_bp_distance]\n",
    "\n",
    "        # Drop all unmappable candidates\n",
    "        ref.dropna(subset = [\"nodeID\"], inplace = True)\n",
    "        ref[\"nodeID\"] = ref[\"nodeID\"].astype(int)\n",
    "\n",
    "        # Drop all SNPs which no longer have a positive case\n",
    "        pos_counts = ref.groupby(\"SNP ID\")[\"Class\"].sum()\n",
    "        ref = ref[~ref[\"SNP ID\"].isin(pos_counts[pos_counts == 0].index)]\n",
    "        \n",
    "        f = ref.groupby(\"nodeID\")[\"Class\"].sum()\n",
    "        f[f > 1] = 1\n",
    "        f = pd.DataFrame(f)\n",
    "\n",
    "        f[\"degree\"] = [degrees[x] for x in list(f.index)]\n",
    "        f[\"1N index\"] = [calculate1N(x, set(f.index[f[\"Class\"] == 1]), G) for x in list(f.index)]\n",
    "        f[\"2N index\"] = [calculate2N(x, set(f.index[f[\"Class\"] == 1]), G) for x in list(f.index)]\n",
    "        f[\"Average DP Distance\"] = [getAverageDPDistance(x, set(f.index[f[\"Class\"] == 1]), G) for x in list(f.index)]\n",
    "        f[\"Topology coefficient\"] = [calculateTopologyCoeff(x, set(f.index[f[\"Class\"] == 1]), G) for x in list(f.index)]\n",
    "        \n",
    "    distance_history = am_values[\"bp distance\"]\n",
    "\n",
    "    outcomes = pd.DataFrame()\n",
    "    train_auc_score = []\n",
    "    train_auc_rank = []\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "    classifier = am_values[\"algorithm\"]\n",
    "    \n",
    "    # Perform leave-SNP-out cross validation\n",
    "    SNPs = list(set(ref[\"SNP ID\"]))\n",
    "    for snp in SNPs:\n",
    "        #print(\"Predicting candidates for \" + snp + \", number \" + str(SNPs.index(snp) + 1) + \" out of \" + str(len(SNPs)))\n",
    "\n",
    "        f_test = f[f.index.isin(ref[ref[\"SNP ID\"] == snp][\"nodeID\"])].copy()\n",
    "        f_train = f[~f.index.isin(f_test.index)].copy()\n",
    "\n",
    "        train_class = f[\"Class\"][f.index.isin(f_train.index)]\n",
    "        test_class = f[\"Class\"][f.index.isin(f_test.index)]\n",
    "\n",
    "        f_test.drop(columns = [\"Class\"], inplace = True)\n",
    "        f_train.drop(columns = [\"Class\"], inplace = True)\n",
    "\n",
    "        if classifier == \"SVM\":\n",
    "            clf = SVR(gamma=\"auto\")\n",
    "        if classifier == \"DT\":\n",
    "            clf = DecisionTreeRegressor()\n",
    "        if classifier == \"KNN1\":\n",
    "            clf = KNeighborsRegressor(n_neighbors = 1)\n",
    "        if classifier == \"KNN3\":\n",
    "            clf = KNeighborsRegressor(n_neighbors = 3)\n",
    "        if classifier == \"KNN5\":\n",
    "            clf = KNeighborsRegressor(n_neighbors = 5)\n",
    "        if classifier == \"KNN7\":\n",
    "            clf = KNeighborsRegressor(n_neighbors = 7)\n",
    "        if classifier == \"KNN9\":\n",
    "            clf = KNeighborsRegressor(n_neighbors = 9)\n",
    "        if classifier == \"LR\":\n",
    "            from warnings import filterwarnings\n",
    "            filterwarnings('ignore')\n",
    "            clf = LogisticRegression()\n",
    "        if classifier == \"RF\":\n",
    "            clf = RandomForestRegressor(n_estimators = 1000, n_jobs = -1, max_features = \"sqrt\", max_depth = 5)\n",
    "\n",
    "        clf.fit(f_train, train_class)\n",
    "\n",
    "        outcomes = pd.concat([outcomes, pd.DataFrame({\"predicted\" : clf.predict(f_test),\n",
    "                                                        \"SNP ID\" : snp,\n",
    "                                                        \"nodeID\" : f_test.index})])\n",
    "\n",
    "    outcomes = outcomes.merge(ref[[\"SNP ID\", \"nodeID\", \"Class\"]], on = [\"SNP ID\", \"nodeID\"], how = \"left\")\n",
    "    \n",
    "    outcomes = outcomes.sort_values([\"SNP ID\", \"predicted\"], ascending = False)\n",
    "    outcomes[\"For-SNP rank\"] = outcomes.groupby(\"SNP ID\").cumcount() + 1\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "    all_metrics.at[am_index, \"Recall snps\"] = len(set(outcomes[\"SNP ID\"]))\n",
    "    all_metrics.at[am_index, \"Recall genes\"] = sum(outcomes[\"Class\"])\n",
    "\n",
    "\n",
    "    import sklearn.metrics\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcomes[\"Class\"], -outcomes[\"For-SNP rank\"], pos_label = 1)\n",
    "    all_metrics.at[am_index, \"ROC-AUC overall (lso)\"] = sklearn.metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "\n",
    "\n",
    "    # Calculate the ROC-AUC for every SNP and average the result\n",
    "    SNPS2 = list(set(outcomes[\"SNP ID\"]))\n",
    "    aucs = []\n",
    "    for snp in SNPS2:\n",
    "      if len(set(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp])) == 1:\n",
    "          aucs.append(list(set(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp]))[0])\n",
    "      else:\n",
    "          fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp], -outcomes[\"For-SNP rank\"][outcomes[\"SNP ID\"] == snp], pos_label = 1)\n",
    "          aucs.append(sklearn.metrics.auc(fpr, tpr))\n",
    "    all_metrics.at[am_index, \"ROC-AUC - mean per snpl (lso)\"] = sum(aucs)/len(aucs)\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    # Calculate hits @1\n",
    "    all_metrics.at[am_index, \"Hits@1(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] == 1)])\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "    # Calculate hits @3\n",
    "    all_metrics.at[am_index, \"Hits@3(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 3)])\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "\n",
    "    # Calculate hits @5\n",
    "    all_metrics.at[am_index, \"Hits@5(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 5)])\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    # Calculate hits @10\n",
    "    all_metrics.at[am_index, \"Hits@10(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 10)])\n",
    "\n",
    "\n",
    "    # In[26]:\n",
    "\n",
    "\n",
    "    all_metrics.at[am_index, \"Mean rank (lso)\"] = outcomes[\"For-SNP rank\"][(outcomes[\"Class\"] == 1)].mean()\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "\n",
    "    all_metrics.at[am_index, \"Median rank (lso)\"] = outcomes[\"For-SNP rank\"][outcomes[\"Class\"] == 1].quantile(q = [0,0.25,0.5,0.75,1])[0.50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.to_csv(\"/Users/vlietstraw/git//Post-GWAS/Network statistics/Leave-SNP-out all metrics \" + datetime.today().strftime(\"%d-%m-%Y\") + \".csv\", sep = \";\", decimal = \",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
