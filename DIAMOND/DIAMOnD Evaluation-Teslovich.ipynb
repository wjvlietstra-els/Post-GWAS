{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the DIAMOnD algorithm to prioritize candidates for every Chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "import pandas as pd\n",
    "import os, json\n",
    "from itertools import product\n",
    "import networkx as nx\n",
    "import scipy.stats\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_and_degrees(G):\n",
    "\n",
    "    neighbors,all_degrees = {},{}\n",
    "    for node in G.nodes():\n",
    "        nn = set(G.neighbors(node))\n",
    "        neighbors[node] = nn\n",
    "        all_degrees[node] = G.degree(node)\n",
    "\n",
    "    return neighbors,all_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_gamma_ln(N):\n",
    "    \"\"\"\n",
    "    precomputes all logarithmic gammas\n",
    "    \"\"\"\n",
    "    gamma_ln = {}\n",
    "    for i in range(1,N+1):\n",
    "        gamma_ln[i] = scipy.special.gammaln(i)\n",
    "\n",
    "    return gamma_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_not_in_cluster_nodes(all_degrees,neighbors,G,not_in_cluster,cluster_nodes,alpha):\n",
    "    reduced_not_in_cluster = {}\n",
    "    kb2k = defaultdict(dict)\n",
    "    for node in not_in_cluster:\n",
    "\n",
    "        k = all_degrees[node]\n",
    "        kb = 0\n",
    "        # Going through all neighbors and counting the number of module neighbors\n",
    "        for neighbor in neighbors[node]:\n",
    "            if neighbor in cluster_nodes:\n",
    "                kb += 1\n",
    "\n",
    "        #adding wights to the the edges connected to seeds\n",
    "        k += (alpha-1)*kb\n",
    "        kb += (alpha-1)*kb\n",
    "        kb2k[kb][k] =node\n",
    "\n",
    "    # Going to choose the node with largest kb, given k\n",
    "    k2kb = defaultdict(dict)\n",
    "    for kb,k2node in kb2k.items():\n",
    "        min_k = min(k2node.keys())\n",
    "        node = k2node[min_k]\n",
    "        k2kb[min_k][kb] = node\n",
    "\n",
    "    for k,kb2node in k2kb.items():\n",
    "        max_kb = max(kb2node.keys())\n",
    "        node = kb2node[max_kb]\n",
    "        reduced_not_in_cluster[node] =(max_kb,k)\n",
    "\n",
    "    return reduced_not_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvalue(kb, k, N, s, gamma_ln):\n",
    "    \"\"\"\n",
    "    -------------------------------------------------------------------\n",
    "    Computes the p-value for a node that has kb out of k links to\n",
    "    seeds, given that there's a total of s sees in a network of N nodes.\n",
    "\n",
    "    p-val = \\sum_{n=kb}^{k} HypergemetricPDF(n,k,N,s)\n",
    "    -------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    p = 0.0\n",
    "    for n in range(kb,k+1):\n",
    "        if n > s:\n",
    "            break\n",
    "        prob = gauss_hypergeom(n, s, N-s, k, gamma_ln)\n",
    "        # print prob\n",
    "        p += prob\n",
    "\n",
    "    if p > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_hypergeom(x, r, b, n, gamma_ln):\n",
    "    return np.exp(logchoose(r, x, gamma_ln) +\n",
    "                  logchoose(b, n-x, gamma_ln) -\n",
    "                  logchoose(r+b, n, gamma_ln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logchoose(n, k, gamma_ln):\n",
    "    if n-k+1 <= 0:\n",
    "        return scipy.infty\n",
    "    lgn1  = gamma_ln[n+1]\n",
    "    lgk1  = gamma_ln[k+1]\n",
    "    lgnk1 = gamma_ln[n-k+1]\n",
    "    return lgn1 - [lgnk1 + lgk1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diamond_iteration_of_first_X_nodes(G,S,X,alpha):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    - G:     graph\n",
    "    - S:     seeds\n",
    "    - X:     the number of iterations, i.e only the first X gened will be\n",
    "             pulled in\n",
    "    - alpha: seeds weight\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    - added_nodes: ordered list of nodes in the order by which they\n",
    "      are agglomerated. Each entry has 4 info:\n",
    "\n",
    "      * name : dito\n",
    "      * k    : degree of the node\n",
    "      * kb   : number of +1 neighbors\n",
    "      * p    : p-value at agglomeration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N = G.number_of_nodes()\n",
    "\n",
    "    added_nodes = []\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Setting up dictionaries with all neighbor lists\n",
    "    # and all degrees\n",
    "    # ------------------------------------------------------------------\n",
    "    neighbors,all_degrees = get_neighbors_and_degrees(G)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Setting up initial set of nodes in cluster\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    cluster_nodes = set(S)\n",
    "    not_in_cluster = set()\n",
    "    s0 = len(cluster_nodes)\n",
    "\n",
    "    s0 += (alpha-1)*s0\n",
    "    N +=(alpha-1)*s0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # precompute the logarithmic gamma functions\n",
    "    # ------------------------------------------------------------------\n",
    "    gamma_ln = compute_all_gamma_ln(N+1)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Setting initial set of nodes not in cluster\n",
    "    # ------------------------------------------------------------------\n",
    "    for node in cluster_nodes:\n",
    "        not_in_cluster |= neighbors[node]\n",
    "    not_in_cluster -= cluster_nodes\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #\n",
    "    # M A I N     L O O P\n",
    "    #\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    all_p = {}\n",
    "\n",
    "    while len(added_nodes) < X:\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        #\n",
    "        # Going through all nodes that are not in the cluster yet and\n",
    "        # record k, kb and p\n",
    "        #\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        pmin = 10\n",
    "        next_node = 'nix'\n",
    "        reduced_not_in_cluster = reduce_not_in_cluster_nodes(all_degrees,\n",
    "                                                             neighbors,G,\n",
    "                                                             not_in_cluster,\n",
    "                                                             cluster_nodes,alpha)\n",
    "\n",
    "        for node,kbk in reduced_not_in_cluster.items():\n",
    "            # Getting the p-value of this kb,k\n",
    "            # combination and save it in all_p, so computing it only once!\n",
    "            kb,k = kbk\n",
    "            try:\n",
    "                p = all_p[(k,kb,s0)]\n",
    "            except KeyError:\n",
    "                p = pvalue(kb, k, N, s0, gamma_ln)\n",
    "                all_p[(k,kb,s0)] = p\n",
    "\n",
    "            # recording the node with smallest p-value\n",
    "            if p < pmin:\n",
    "                pmin = p\n",
    "                next_node = node\n",
    "\n",
    "            info[node] = (k,kb,p[0])\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Adding node with smallest p-value to the list of aaglomerated nodes\n",
    "        # ---------------------------------------------------------------------\n",
    "        added_nodes.append((next_node,\n",
    "                            info[next_node][0],\n",
    "                            info[next_node][1],\n",
    "                            info[next_node][2]))\n",
    "\n",
    "        # Updating the list of cluster nodes and s0\n",
    "        cluster_nodes.add(next_node)\n",
    "        s0 = len(cluster_nodes)\n",
    "        not_in_cluster |= ( neighbors[next_node] - cluster_nodes )\n",
    "        not_in_cluster.remove(next_node)\n",
    "\n",
    "    return added_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the edgelist to create the network\n",
    "G = nx.read_edgelist(\"/Users/vlietstraw/git/Post-GWAS/unfiltered_protein_protein_interactions.csv\", delimiter = \",\", nodetype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping file\n",
    "with open(\"/Users/vlietstraw/git/Post-GWAS/ENSEMBL_mappings.json\", \"r\") as fp:\n",
    "    ensembl_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_bp_distances = [25, 50, 100, 500, 1000, 2000]\n",
    "all_bp_distances = [25, 50, 100, 500, 1000, 2000, \"depict\"]\n",
    "refsets = [\"Teslovich\"]#, \"DeRycke\", \"farashi\", \"farashi p-value cutoff\"]\n",
    "\n",
    "all_metrics = pd.DataFrame(list(product(refsets, all_bp_distances)), columns = [\"refset\", \"bp distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting row 1 of 7\n",
      "Getting candidates for chromosome 16\n",
      "Getting candidates for chromosome 1\n",
      "Getting candidates for chromosome 6\n"
     ]
    }
   ],
   "source": [
    "nCandidates = 1000\n",
    "    \n",
    "for am_index, am_values in all_metrics.iterrows():\n",
    "    outcomes = pd.DataFrame()\n",
    "    outcomes2 = pd.DataFrame()\n",
    "    \n",
    "    print(\"Predicting row \" + str(am_index + 1) + \" of \" + str(len(all_metrics)))\n",
    "    if am_values[\"refset\"] == \"farashi\":\n",
    "        ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Farashi/Farashi full 2000000 bp distance no pvalue filtering.csv\")\n",
    "\n",
    "    if am_values[\"refset\"] == \"farashi p-value cutoff\":\n",
    "        ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Farashi/Farashi full 2000000 bp distance no pvalue filtering.csv\")\n",
    "        ref = ref[ref[\"GWAS/eQTL p-valueÂ¥\"] <= float(\"5e-8\")]\n",
    "\n",
    "    if am_values[\"refset\"] == \"DeRycke\":\n",
    "        ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/DeRycke/DeRycke reference set.csv\")\n",
    "        ref.columns = [\"SNP ID\", \"chromosome\", \"location\", \"gene_ids\", \"gene name\", \"gene start\", \"gene stop\", \"Diff expression\", \"Class\", \"bp distance absolute\", \"bp distance\", \"Gene rank\"]\n",
    "\n",
    "    if am_values[\"refset\"] == \"Teslovich\":\n",
    "        ref = pd.read_csv(\"/Users/vlietstraw/git/Post-GWAS/Input sets/Teslovich/Teslovich reference set.csv\")\n",
    "        ref.columns = [\"SNP ID\", \"chromosome\", \"location\", \"P\", \"gene_ids\", \"gene name\", \"gene start\", \"gene stop\", \"Class\", \"bp distance absolute\", \"bp distance\", \"Gene rank\"]\n",
    "\n",
    "    ref[\"nodeID\"] = [ensembl_dict[x] if x in ensembl_dict.keys() else None for x in ref[\"gene_ids\"]]\n",
    "\n",
    "    # Set bp distance cutoff\n",
    "    if am_values[\"bp distance\"] != \"depict\":\n",
    "        max_bp_distance = am_values[\"bp distance\"]\n",
    "        max_bp_distance = max_bp_distance * 1000\n",
    "        ref = ref[ref[\"bp distance absolute\"] <= max_bp_distance]\n",
    "    elif am_values[\"bp distance\"] == \"depict\":\n",
    "        depict = pd.read_csv(\"~/git/DEPICT/outcomes/Teslovich for paper Wytze/Teslovich_output_geneprioritization.txt\", sep = \"\\t\")\n",
    "        depict[\"nodeID\"] = [ensembl_dict[x] if x in ensembl_dict.keys() else None for x in depict[\"Ensembl gene ID\"]]\n",
    "        \n",
    "        depict[\"Locus\"] = depict[\"Locus\"].astype(str).apply(lambda x: x.split(\";\"))\n",
    "        depict = depict.explode(\"Locus\")\n",
    "\n",
    "        snp_replacement_dict = {\"rs113645266\" : \"rs6557271\",\n",
    "                        \"rs150282463\" : \"rs13137700\",\n",
    "                        \"rs67276543\" : \"rs34884832\"}\n",
    "        depict[\"Locus\"] = depict[\"Locus\"].replace(snp_replacement_dict)\n",
    "\n",
    "        depict = depict[[\"Locus\", \"nodeID\"]]\n",
    "        depict.columns = [\"SNP ID\", \"nodeID\"]\n",
    "\n",
    "        ref = ref.merge(depict, on = [\"SNP ID\", \"nodeID\"], how = \"inner\")\n",
    "\n",
    "    # Drop all unmappable candidates\n",
    "    ref.dropna(subset = [\"nodeID\"], inplace = True)\n",
    "    ref[\"nodeID\"] = ref[\"nodeID\"].astype(int)\n",
    "\n",
    "    # Drop all SNPs which no longer have a positive case\n",
    "    pos_counts = ref.groupby(\"SNP ID\")[\"Class\"].sum()\n",
    "    ref = ref[~ref[\"SNP ID\"].isin(pos_counts[pos_counts == 0].index)]\n",
    "    \n",
    "#    SNPs = list(set(ref[\"SNP ID\"]))\n",
    "#    for snp in SNPs:\n",
    "#        print(\"Predicting candidates for \" + snp + \", number \" + str(SNPs.index(snp) + 1) + \" out of \" + str(len(SNPs)))\n",
    "#        out = pd.DataFrame(diamond_iteration_of_first_X_nodes(G, ref[(ref[\"SNP ID\"] != snp) & (ref[\"Class\"] == 1)][\"nodeID\"], nCandidates, 1), \n",
    "#                   columns = [\"nodeID\", \"unknown1\", \"unknown2\", \"predicted\"])\n",
    "#        out[\"SNP ID\"] = snp\n",
    "#        out = out[out[\"nodeID\"].isin(ref[\"nodeID\"][ref[\"SNP ID\"] == snp])]\n",
    "#        out[\"Class\"] = [1 if x in list(ref[\"nodeID\"][(ref[\"Class\"] == 1) & (ref[\"SNP ID\"] == snp)]) else 0 for x in out[\"nodeID\"]]\n",
    "#        outcomes = outcomes.append(out) # predicted is a p-value, so lower is better\n",
    "       \n",
    "\n",
    "    \n",
    "    chromosomes = list(set(ref[\"chromosome\"]))\n",
    "    for chrom in chromosomes:\n",
    "        print(\"Getting candidates for chromosome \" + str(chrom))\n",
    "        # Write away the seed to an input file\n",
    "        out = pd.DataFrame(diamond_iteration_of_first_X_nodes(G, ref[(ref[\"chromosome\"] != chrom) & (ref[\"Class\"] == 1)][\"nodeID\"], nCandidates, 1), \n",
    "                           columns = [\"nodeID\", \"unknown1\", \"unknown2\", \"predicted\"]) # predicted is a p-value, so lower is better\n",
    "        out[\"chromosome\"] = chrom\n",
    "        out[\"Class\"] = [1 if x in list(ref[\"nodeID\"][ref[\"Class\"] == 1]) else 0 for x in out[\"nodeID\"]]\n",
    "        out = out[out[\"nodeID\"].isin(ref[\"nodeID\"][ref[\"chromosome\"] == chrom])]\n",
    "        outcomes2 = outcomes2.append(out)\n",
    "        \n",
    "  #  outcomes = outcomes.sort_values([\"SNP ID\", \"predicted\"], ascending = True)\n",
    "  #  outcomes[\"For-SNP rank\"] = outcomes.groupby(\"SNP ID\").cumcount() + 1\n",
    "\n",
    "  #  all_metrics.at[am_index, \"Recall snps\"] = len(set(outcomes[\"SNP ID\"]))\n",
    "  #  all_metrics.at[am_index, \"Recall genes\"] = sum(outcomes[\"Class\"])\n",
    "        \n",
    "  #  fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcomes[\"Class\"], -outcomes[\"For-SNP rank\"], pos_label = 1)\n",
    "  #  all_metrics.at[am_index, \"ROC-AUC overall (lso)\"] = sklearn.metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "#    # Calculate the ROC-AUC for every SNP and average the result\n",
    "#    SNPS2 = list(set(outcomes[\"SNP ID\"]))\n",
    "#    aucs = []\n",
    "#    for snp in SNPS2:\n",
    "#        if len(set(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp])) == 1:\n",
    "#            aucs.append(list(set(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp]))[0])\n",
    "#        else:\n",
    "#            fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcomes[\"Class\"][outcomes[\"SNP ID\"] == snp], -outcomes[\"For-SNP rank\"][outcomes[\"SNP ID\"] == snp], pos_label = 1)\n",
    "#            aucs.append(sklearn.metrics.auc(fpr, tpr))\n",
    "#    all_metrics.at[am_index, \"ROC-AUC - mean per snpl (lso)\"] = sum(aucs)/len(aucs)\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    # Calculate hits @1\n",
    "#    all_metrics.at[am_index, \"Hits@1(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] == 1)])\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "    # Calculate hits @3\n",
    "#    all_metrics.at[am_index, \"Hits@3(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 3)])\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "\n",
    "    # Calculate hits @5\n",
    "#    all_metrics.at[am_index, \"Hits@5(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 5)])\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    # Calculate hits @10\n",
    "#    all_metrics.at[am_index, \"Hits@10(lso)\"] = sum(outcomes[\"Class\"][(outcomes[\"Class\"] == 1) & (outcomes[\"For-SNP rank\"] <= 10)])\n",
    "\n",
    "\n",
    "    # In[26]:\n",
    "\n",
    "\n",
    "#    all_metrics.at[am_index, \"Mean rank (lso)\"] = outcomes[\"For-SNP rank\"][(outcomes[\"Class\"] == 1)].mean()\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "\n",
    "#    all_metrics.at[am_index, \"Median rank (lso)\"] = outcomes[\"For-SNP rank\"][outcomes[\"Class\"] == 1].quantile(q = [0,0.25,0.5,0.75,1])[0.50]\n",
    "\n",
    "\n",
    "    # ## Evaluate leave-chromosome-out\n",
    "\n",
    "    # In[28]:\n",
    "\n",
    "\n",
    "    outcomes2 = outcomes2.sort_values([\"chromosome\", \"predicted\"])\n",
    "    outcomes2[\"For-chromosome rank\"] = outcomes2.groupby(\"chromosome\").cumcount() + 1\n",
    "\n",
    "\n",
    "    # In[29]:\n",
    "\n",
    "\n",
    "    #chromosomes = list(set(outcomes2[\"chromosome\"]))\n",
    "    #aucs = []\n",
    "    #for chrom in chromosomes:\n",
    "    #  fpr, tpr, thresholds = sklearn.metrics.roc_curve(outcomes2[\"Class\"][outcomes2[\"chromosome\"] == chrom], -outcomes2[\"For-chromosome rank\"][outcomes2[\"chromosome\"] == chrom], pos_label = 1)\n",
    "    #  aucs.append(sklearn.metrics.auc(fpr, tpr))\n",
    "    #print(sum(aucs)/len(aucs))\n",
    "\n",
    "\n",
    "    # In[30]:\n",
    "\n",
    "\n",
    "    ref = ref.merge(outcomes2[[\"nodeID\", \"predicted\"]], on = \"nodeID\")#, how = \"left\")\n",
    "    pos_counts = ref.groupby(\"SNP ID\")[\"Class\"].sum()\n",
    "    ref = ref[~ref[\"SNP ID\"].isin(pos_counts[pos_counts == 0].index)]\n",
    "    \n",
    "    if len(ref) > 0:\n",
    "        all_metrics.at[am_index, \"Recall snps (lco)\"] = len(set(ref[\"SNP ID\"]))\n",
    "        all_metrics.at[am_index, \"Recall genes (lco)\"] = sum(ref[\"Class\"])\n",
    "\n",
    "        # In[31]:\n",
    "\n",
    "\n",
    "        ref = ref.sort_values([\"SNP ID\", \"predicted\"], ascending = True)\n",
    "        ref[\"For-SNP rank\"] = ref.groupby(\"SNP ID\").cumcount() + 1\n",
    "\n",
    "\n",
    "        # In[32]:\n",
    "\n",
    "\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(ref[\"Class\"], -ref[\"For-SNP rank\"], pos_label = 1)\n",
    "        all_metrics.at[am_index, \"ROC-AUC overall (lco)\"] = sklearn.metrics.auc(fpr, tpr) * 100\n",
    "\n",
    "\n",
    "        # In[33]:\n",
    "\n",
    "\n",
    "        # Calculate the ROC-AUC for every SNP and average the result\n",
    "        SNPS2 = list(set(ref[\"SNP ID\"]))\n",
    "        aucs = []\n",
    "        for snp in SNPS2:\n",
    "          if len(set(ref[\"Class\"][ref[\"SNP ID\"] == snp])) == 1:\n",
    "              aucs.append(list(set(ref[\"Class\"][ref[\"SNP ID\"] == snp]))[0])\n",
    "          else:\n",
    "              fpr, tpr, thresholds = sklearn.metrics.roc_curve(ref[\"Class\"][ref[\"SNP ID\"] == snp], -ref[\"For-SNP rank\"][ref[\"SNP ID\"] == snp], pos_label = 1)\n",
    "              aucs.append(sklearn.metrics.auc(fpr, tpr))\n",
    "        all_metrics.at[am_index, \"ROC-AUC - mean per snpl (lco)\"] = sum(aucs)/len(aucs)\n",
    "\n",
    "\n",
    "        # In[34]:\n",
    "\n",
    "\n",
    "        # Calculate hits @1\n",
    "        all_metrics.at[am_index, \"Hits@1(lco)\"] = sum(ref[\"Class\"][(ref[\"Class\"] == 1) & (ref[\"For-SNP rank\"] == 1)])\n",
    "\n",
    "\n",
    "        # In[35]:\n",
    "\n",
    "\n",
    "        # Calculate hits @3\n",
    "        all_metrics.at[am_index, \"Hits@3(lco)\"] = sum(ref[\"Class\"][(ref[\"Class\"] == 1) & (ref[\"For-SNP rank\"] <= 3)])\n",
    "\n",
    "\n",
    "        # In[36]:\n",
    "\n",
    "\n",
    "        # Calculate hits @5\n",
    "        all_metrics.at[am_index, \"Hits@5(lco)\"] = sum(ref[\"Class\"][(ref[\"Class\"] == 1) & (ref[\"For-SNP rank\"] <= 5)])\n",
    "\n",
    "\n",
    "        # In[37]:\n",
    "\n",
    "\n",
    "        # Calculate hits @10\n",
    "        all_metrics.at[am_index, \"Hits@10(lco)\"] = sum(ref[\"Class\"][(ref[\"Class\"] == 1) & (ref[\"For-SNP rank\"] <= 10)])\n",
    "\n",
    "\n",
    "        # In[38]:\n",
    "\n",
    "\n",
    "        all_metrics.at[am_index, \"Mean rank (lco)\"] = ref[\"For-SNP rank\"][(ref[\"Class\"] == 1)].mean()\n",
    "\n",
    "\n",
    "        # In[39]:\n",
    "\n",
    "\n",
    "        all_metrics.at[am_index, \"Median rank (lco)\"] = ref[\"For-SNP rank\"][ref[\"Class\"] == 1].quantile(q = [0,0.25,0.5,0.75,1])[.50]\n",
    "    \n",
    "    outcomes2.to_csv(\"/Users/vlietstraw/git/Post-GWAS/DIAMOND/\" + am_values[\"refset\"] + \" diamond predictions with bp distance \" + str(am_values[\"bp distance\"]) + \".csv\", sep = \";\", decimal = \",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.to_csv(\"/Users/vlietstraw/git/Post-GWAS/DIAMOND/all_variations_performance_metrics \" + am_values[\"refset\"] + \" \" + datetime.today().strftime(\"%d-%m-%Y\") +\".csv\", sep = \";\", decimal = \",\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
